---
title: "Chapter 02: Levels of Explanation and Brain Metaphors"
---

# Levels of Explanation and Brain Metaphors

## Introduction  

This is a course about the **brain**. We will consider the constituent cells that comprise it and the anatomical structures and connections they form. But the course is equally about brain function. With an organ such as the heart, the relationship between form and function is relatively obvious: it is built to pump blood, and its anatomy makes that role clear. No such straightforward mapping exists for the brain. We might presume that its function is to **control behavior**, but what do we really mean by control? And what do we mean by behavior? Is it limited to moving the body in space to achieve a goal, or does it also include internal processes such as mental arithmetic, imagination, or self-reflection—phenomena that leave no direct motor trace but are central to what we call mind?  

The difficulty of defining what counts as “control” or “behavior” leads directly into larger questions about how we explain the brain. Should we rely on behavioral constructs such as attention or memory, even if they risk confusing description with mechanism? Should we borrow metaphors from current technology, knowing they illuminate some aspects of brain function while obscuring others? These are the kinds of issues we will wrestle with in this chapter as we examine levels of analysis and the metaphors that have shaped neuroscience.

Because these debates are shaped by one’s assumptions about mind and brain, I want to be transparent about the orientation I bring to them. The following note sketches the perspective that guides how I approach neuroscience in this course.

## Author’s Biases: Materialism, Reductionism, and Complexity

::: {.callout-note}
### My approach to science

I want to be transparent about my theoretical orientation from the outset. I am a **materialist**: I believe that the mind emerges from the physiological activities of the brain. Many students—without realizing it—carry an intuitive form of **dualism**, the idea that mind and body are separate. Dualism has an important place in the history of philosophy, and some scientists hold dualist views privately. But in neuroscience it is not a useful stance. To do science we rely on **methodological materialism**: whatever our personal beliefs, we seek explanations of thought and behavior in terms of brain and body.

I am also sometimes described as a **reductionist**, though a more accurate label might be **integrative reductionist** or **non-reductive physicalist**. By this I mean that psychological phenomena are, in the end, the activity of the brain. Explanations at the psychological level must remain consistent with biology, just as chemistry must remain consistent with physics. But “neural activity” is not limited to individual spikes—it includes the interactions of circuits, the dynamics of networks, and the emergent properties of complex systems. That is why higher-level models are often indispensable, even though they remain firmly grounded in the biology of the brain.

If there is any “special sauce” in neuroscience, it is **complexity**. Brains—like weather systems or ecosystems—produce rich behaviors through countless nonlinear interactions. Such systems often demand **probabilistic** or **dynamical** models rather than simple cause-and-effect rules. Complexity does not cancel reductionism; it shows why higher-level models are indispensable, even as they remain biologically grounded.

This is the orientation I bring to neuroscience: a commitment to **materialism**, biological constraint, and respect for the complexity from which mind emerges.

### Glossary

- **Materialism (physicalism)** — The view that mind and consciousness arise from the physical activity of the brain.  
- **Dualism** — The belief that mind and body are fundamentally separate realities.  
- **Methodological materialism** — The practice of explaining phenomena in natural, physical terms when doing science, regardless of personal beliefs.  
- **Reductionism** — The idea that higher-level theories must remain consistent with lower-level principles (e.g., psychology with biology).  
- **Emergence** — Novel properties that arise from the interaction of simpler elements.  
- **Complexity** — Behaviors of a system not predictable by analyzing its parts in isolation.  
- **Chaos** — Sensitivity to initial conditions, where small differences can yield large, unpredictable outcomes.  
- **Dynamical systems** — Frameworks for describing how systems evolve over time through interaction and feedback.  
- **Statistical description** — Using averages, distributions, and probabilities to capture the behavior of complex systems.  
:::

## Levels of Analysis: The Problem of Category Errors

One of the central challenges in linking brain structure to function is the risk of confusing **levels of analysis**. Concepts that are useful at the behavioral level are often reified as if they were literal brain mechanisms. This is especially problematic with constructs such as attention, working memory, or executive function—terms inferred from behavior rather than observed directly.  

Different approaches have been taken to explore the brain–behavior relationship. **Cognitive psychology** developed constructs to describe regularities in performance. **Cognitive neuroscience**, especially with the advent of neuroimaging, often asks where such constructs might be instantiated in the brain. **Cognitive neuropsychology**, working with patients who had focal brain lesions, used methods such as **double dissociation** to infer functional architecture. And **behavioral neuroscience**, often using experimental animal models, sought to link neural manipulations directly to observable behavior, usually in paradigms such as learning, motivation, or sensory discrimination. Each approach yielded insights, but each also risked blurring the line between behavioral description and neural mechanism.  

This is the danger of a category error. It is like mistaking the **map** for the **territory**. A map is a powerful tool for navigation, but it is not the landscape itself. Similarly, behavioral constructs organize and predict performance, but they are not the mechanisms that generate it. The fact that “maps” are useful at the behavioral level does not mean the brain must literally implement them as cartographic layouts. Place cells and grid cells, for example, support navigation through population codes that look nothing like a two-dimensional atlas. The lesson is that behavioral categories are not blueprints of neural architecture.  

To be clear, cognitive psychology itself did not make this mistake. When Alan Baddeley proposed his model of working memory in the 1970s, he was explicit that it was a **behavioral theory**. The phonological loop, visuospatial sketchpad, and central executive were abstractions designed to explain regularities such as rehearsal span or selective interference. At that level, the model was enormously successful. Still, concepts from cognitive psychology are often aligned with computational models and not constrained by biological mechanisms—a theme we will return to when discussing the computer metaphor in a later section. The trouble began when cognitive neuroscience, equipped with new imaging tools, treated such behavioral models as if they must have direct neural counterparts, asking where the phonological loop or central executive might be located in the brain.  

**Cognitive neuropsychology**, working with patients who had focal brain lesions, defined cognitive functions in behavioral terms and tested whether they could be dissociated. At Queen Square and elsewhere, researchers such as Elizabeth Warrington and Tim Shallice showed that certain abilities could break down independently (double dissociation): for example, recognising objects vs. faces, processing color vs. motion, or understanding nouns vs. verbs. These results supported the idea of distinct cognitive modules, but it is important to remember that the categories being tested were psychological constructs. Detailed anatomical or mechanistic mappings were often assumed rather than directly demonstrated.  

::: {.callout-note collapse="true"}  
### Deeper Dive: Understanding Double Dissociations

A dissociation occurs when brain damage impairs one ability while sparing another. Such cases are striking, but a single dissociation may only show that one task is harder or more fragile than another. Double dissociations strengthen the inference by showing complementary patterns across patients: lesion A impairs X but spares Y, while lesion B impairs Y but spares X.  

A classic example comes from memory research. Patients with Korsakoff’s syndrome suffer profound long-term memory deficits yet often show normal digit span. By contrast, the patient KF, studied by Warrington and Shallice, could form new long-term memories but had a drastically reduced digit span. Taken together, these patterns suggested distinct short-term and long-term memory systems.  

This interpretation proved influential, but modern neuroscience offers a different view. Short-term memory often reflects sustained activity in perceptual circuits, while long-term memory involves synaptic changes in those same circuits. KF’s impairment may have reflected damage to left-hemisphere language pathways rather than a dedicated “short-term store.” The dissociations were real, but the categories they seemed to validate were behavioral constructs, not necessarily the brain’s own divisions.  

The logic of dissociation has since been extended to neuroimaging. For example, an area in the fusiform gyrus responds more strongly to faces than to places, while a nearby area in the parahippocampal gyrus shows the opposite pattern. This constitutes a **functional double dissociation**: region A is more active for task X than Y, and region B more active for task Y than X. These results are compelling, but it is important to note the distinction: lesions demonstrate **necessity** (the region is required for the function), while imaging demonstrates **association** or **preference** (the region is more engaged by the function). Both approaches are powerful, but they answer different questions.   

---

**Glossary**  
- **Double dissociation** — Evidence that two functions are independent, shown when each can be impaired without affecting the other.  
- **Functional double dissociation (imaging)** — A pattern in which one brain region shows stronger activity for task A than B, and another shows the reverse.  
- **Fusiform gyrus** — A convolution of the ventral temporal lobe, running along the basal surface, often implicated in high-level visual processing.  
- **Parahippocampal gyrus** — A convolution of the medial temporal lobe, adjacent to the hippocampus, involved in contextual and spatial processing.  
- **Necessity vs. association** — Lesions show whether a region is required for a function (necessity); imaging shows whether it tends to be involved (association).  
:::

---

## Historical Metaphors: From Pneumatics to Computers

If levels of analysis shape the categories we use, metaphors shape the stories we tell. Throughout history, neuroscientists have drawn on the technologies of their era to explain the brain. Each metaphor was illuminating in its time, but each eventually constrained thought when mistaken for literal truth.  

In antiquity, the dominant technologies were hydraulics and pneumatics. Greeks and Romans, masters of aqueducts and pumps, imagined that vital spirits or pneuma flowed through hollow nerves like water through pipes. Galen’s physiology described the brain as a reservoir that distributed these animal spirits to the body. Muscles, in his account, contracted when infused with pneuma, much as bladders expand when filled. Within the framework of hydraulic engineering, this was a powerful and reasonable model.  

In the seventeenth century, René Descartes extended this pneumatic metaphor with the mechanical imagination of his age. At the gardens of Versailles he observed hydraulic automata: statues that struck, danced, or sprayed water when hidden valves opened. He imagined nerves as pipes carrying “animal spirits” and the pineal gland as the valve controlled by the soul. Muscles contracted, in his view, just as statues moved when water pressure filled their chambers.  

The age of clockwork soon brought a new metaphor. Elaborate automata, powered by springs and gears, captivated audiences. Birds that sang, dolls that played instruments, machines that walked—all demonstrated that intricate motion could arise from mechanism alone. The nervous system seemed, by analogy, to be clockwork: precise, deterministic, composed of interlocking parts.  

By the nineteenth century, the rise of electricity and telegraphy offered another model. Nerves came to be seen as wires conducting signals, the brain as a switchboard routing messages from one place to another. This metaphor was reinforced by discoveries of nerve conduction and action potentials, which did in fact involve electrical impulses, and by the telephone switchboard, which suggested cortical “centers” connected by fibers, with operators routing signals between them.

In the twentieth century, as digital computers transformed science and society, the nervous system was recast once again. The computer metaphor became dominant: neurons as logic gates, circuits as information-processing units, thought as computation. With the von Neumann machine as template, the brain was reimagined as processor, memory, and bus. John von Neumann, the Hungarian-American mathematician, had outlined a simple but powerful design in which a central processing unit executes instructions stored in memory, with both data and instructions traveling along the same bus—an architecture so effective that it remains the foundation of virtually all modern computers. Borrowing from this now-ubiquitous architecture, psychology and the emerging field of cognitive neuroscience embraced the metaphor with particular enthusiasm, treating it as a bridge between brain structure and brain function. The vocabulary of computation—encode, retrieve, buffer, bandwidth—saturated their discourse. Working memory was likened to RAM, attention to bandwidth, consolidation to storage transfer. The metaphor was enormously productive. But like those before it, it risked being mistaken for mechanism rather than scaffold.

As with earlier metaphors, the computer metaphor is not wrong but limited. It belongs to the family of formal sciences—mathematics, logic, computer science, artificial intelligence—that study abstract systems in their own right. Cognitive psychology, especially in its computational turn, often aligns with this tradition. These fields are rigorous and legitimate, but they are not part of the continuum of natural sciences like biology, chemistry, or physics. The brain, as a biological organ shaped by evolution, cannot be understood solely through the lens of formal systems. Computational models may provide useful scaffolds, but they remain metaphors unless constrained by the material workings of neurons and circuits.


## Why Brains Are Not von Neumann Machines

The problem is not that brains differ from physically realized computers in detail, but that they differ in principle. **Von Neumann machines** separate memory from processing, rely on a central processor, and move data along buses. Brains do none of these things.  

In the brain, the same synapses that “store” information also transform it. Memory and processing are inseparable. Each neuron is simultaneously a memory element and a processor, integrating thousands of inputs and influencing thousands of outputs. There is no global bus shuttling data around, and no central executive fetching instructions. The brain’s 86 billion neurons compute in parallel, without a master controller.  

To recall your grandmother’s face is not to fetch a stored file but to reconstruct a new pattern of activity, shaped by current inputs and prior connectivity. The architecture does not implement an algorithm; it *is* the computation.  

This distinction becomes clearer when we contrast simulation with instantiation. A digital computer **simulates** physical processes by representing them symbolically and executing algorithms step by step. By contrast, natural and analog systems often **instantiate** processes directly. A stream does not calculate turbulence—it *is* turbulence. An analog circuit does not simulate an equation—it embodies its dynamics in voltages and currents. In the same way, neural circuits may not simulate perception or control; they may *be* the processes by which organisms perceive and act. As James Gibson argued, perception and action are rooted in resonance with environmental structure, not in symbol manipulation.  

::: {.callout-note}
### Simulation vs. Instantiation

- **Simulation** (digital computers): represent a process symbolically and calculate outcomes step by step.  
- **Instantiation** (natural/analog systems): the process unfolds directly in the physical medium.  
- A stream does not simulate turbulence — it *is* turbulence.  
- Neural circuits may not simulate perception — they may *be* perception and action.  
:::

Seen in this light, the computer metaphor belongs on the same continuum as pneuma, clockwork, and telegraphs. Each was useful, seductive, and misleading when taken literally.  

::: {.callout-note collapse="true"}  
### Deeper Dive: Computer Architectures

Von Neumann machines are general-purpose and can simulate other systems—parallel processors, analog circuits, even neural networks. But simulation is not the same as implementation. A laptop crunching equations of fluid dynamics is not the same as a river flowing downhill.  

Engineers are now building chips that abandon von Neumann’s strict separation of memory and processing. **Neuromorphic** chips (e.g., Intel’s *Loihi*, IBM’s *TrueNorth*) use circuits that mimic networks of spiking neurons, operating asynchronously and in parallel. **Memristive** devices collapse storage and computation into the same substrate, like synapses that both transmit and store. **Analog** computers, once set aside, are re-emerging for problems where continuous dynamics matter.  

These architectures remind us that while the von Neumann computer is currently ubiquitous, it is only one model of computation. There are many ways to build a computer—and the brain may be better understood through models where memory and computation are inseparable, and dynamics arise from the physics of the system itself.  
:::  

---

## Networks, Dynamics, and Embodiment  

In response to the limitations of the computer metaphor, neuroscience embraced the **network metaphor**: brains as collections of interconnected nodes, with information flowing along directed graphs. This was a step forward. It highlighted distributed processing, connectivity, and emergent dynamics.  

But networks can also drift into abstraction. High-dimensional state spaces, manifold trajectories, and attractor dynamics can become detached from embodied reality. The brain is not a free-floating network. It has inputs and outputs, and its purpose is to control a body in a world. As James Gibson argued, perception is about direct engagement with environmental structure, not the construction of inner pictures. Evolution acts on **behavior**, not on representations. The brain is an embodied control system, not a disembodied network.  

## Control Theory: Toward a More Useful Metaphor  

If not a general-purpose computer, then what? For me, a more satisfying metaphor is that of the brain as a **control system**: a layered, adaptive, real-time regulator of behavior. Control theory provides the mathematics for such systems, but the core ideas are intuitive.  

### From Reflex to Prediction  

The simplest control system is reactive. A **thermostat** turns on the furnace when temperature falls below a set point and off when it rises above. Early nervous systems functioned much like thermostats. The lamprey’s brain, for example, coordinates swimming through rhythmic motor circuits — essentially reactive loops.  

But reactive control has limits. Organisms that could anticipate had an advantage. This led to **feedforward control**: using sensory cues to act in advance. A frog leaping at an insect does not wait for feedback; it generates a predictive strike. A mammal running across rough terrain adjusts its gait before stumbling. Feedforward control requires **internal models**: representations of the body and environment that allow prediction.  

Even the thermostat metaphor can be extended. A **Nest thermostat** does not simply react to temperature—it learns from your past behavior and builds an internal model of your preferred comfort levels. It can anticipate when you will want the house warmer or cooler, and it can incorporate contingencies such as whether anyone is home. In this way, it resembles predictive nervous systems that simulate future states to guide action.  

Humans extend this further. We simulate possible futures, weigh alternative actions, and choose among them. We not only react and predict but deliberate. This progression — reflex, feedforward, internal model, deliberation — mirrors the evolutionary arc of nervous systems. Evolution appears to have elaborated simple control architectures into systems with additional layers of regulation and arbitration. Later in the book, we will see how this principle applies to a brain structure called the **basal ganglia**, which shows remarkable continuity from ancient vertebrates to humans.  

::: {.callout-note}  
### Reactive vs. Predictive Control  
- **Feedback (reactive)** — Using the consequences of action to reduce error.  
- **Feedforward (predictive)** — Generating actions from internal prediction, without waiting for error signals.  
- **Internal models** — Learned mappings that forecast how the body and world will respond, enabling anticipation and planning.  
:::  

### Many Variables, One Body  

A thermostat regulates one thing: temperature. A Nest thermostat already hints at a further challenge: arbitration. It must balance comfort against energy use, sometimes accepting less-than-ideal temperatures to reduce cost. Here two systems are coupled — the goal of comfort and the goal of minimizing expense — and the solution to the joint problem is not the same as the solution to either alone.  

Organisms face this challenge multiplied many times over: hunger, thirst, fatigue, thermoregulation, reproduction, defense, social affiliation. These drives cannot all be satisfied simultaneously. The brain’s challenge is not only regulation but **arbitration**: deciding what to regulate now and at what cost to other goals.  

This arbitration emerges from a hierarchical architecture. The hypothalamus and brainstem regulate basic drives. Midbrain and striatal circuits weigh and prioritize. Cortical systems simulate futures, incorporate social context, and bias the balance. Arbitration is not commanded by a central executive but emerges from **competition, neuromodulation, and learning**.

::: {.callout-note}  
### Arbitration Under Constraints  
- Organisms juggle multiple variables at once: hunger, thirst, fatigue, reproduction, safety, and social goals.  
- **Arbitration** is the process of prioritizing among these competing needs.
- In brains, arbitration emerges from layered circuits — hypothalamus, midbrain, striatum, cortex — through competition, neuromodulation, and learning.  

This perspective sets up how we will later discuss control hierarchies in the brain.  
:::  

### Concrete Examples of Arbitration  

A bird suppresses feeding to evade a predator. A parent sacrifices sleep to care for offspring. A human forgoes immediate gratification to invest in a long-term goal. Even exploration and play, which seem frivolous, can be understood as **meta-control strategies**: they expand the model space, improving future arbitration.  

These examples illustrate that the brain’s essence is not symbol manipulation but dynamic balancing under constraint. Cognition, in this view, is control.  

## Conclusion: Bootstrapping with Better Scaffolds  

From pneumatics to computers, each metaphor illuminated the brain in its time. None was foolish; each reflected the tools and intuitions of its era. But metaphors can mislead when mistaken for literal truth. The way forward is to treat them as scaffolds: useful for climbing, but to be discarded once higher ground is reached.  

Control theory offers a scaffold that is biologically grounded. It emphasizes feedback, prediction, and arbitration. It reframes cognitive constructs as emergent properties of control. And it prepares us to consider, in the next chapter, how brains evolved the architectures that make such control possible.

Science progresses not by revelation but by bootstrapping. We climb on imperfect scaffolds, building new ones as we go. Our task is to keep climbing, with humility and care.


## Summary and Questions for Reflection

### What You Should Know  

- Explanations of brain and behavior must respect **levels of analysis**.  
- **Category errors** occur when behavioral constructs are mistaken for neural mechanisms.  
- **Cognitive psychology** provided useful behavioral models but often unconstrained by biology.  
- **Cognitive neuroscience** sometimes risks reifying those models in brain maps.  
- **Cognitive neuropsychology** used lesions to infer modules but relied on psychological constructs.  
- **Behavioral neuroscience** stays closer to observable behavior using animal models, but with limited reach for human cognition.  
- Metaphors (pneumatics, clockwork, telegraphs, computers) have been useful but are dangerous if mistaken for mechanisms.  
- The **computer metaphor** belongs to the formal sciences; neuroscience must remain a **natural science**.  
- A **control systems framework** offers a biologically grounded way to understand how brains generate adaptive behavior.  

### Questions to Ponder  

- If cognitive psychology’s constructs are “just” behavioral abstractions, why have they proven so durable and useful?  
- Could computational models capture something essential about brain function, even if the brain is not literally a computer?  
- Is it too restrictive to insist that psychology must always be biologically constrained to count as science?  
- Are metaphors indispensable for progress in neuroscience, or should we try to avoid them altogether?  
- If brains are best understood as control systems, what important aspects of mind or behavior might that metaphor leave out?  


## General Glossary

- **Attention** — A behavioral construct describing selective prioritization of information.  
- **Behavior** — Observable actions of an organism; sometimes extended to include internal mediating processes.  
- **Category error** — Mistaking a description at one level of analysis for a mechanism at another.  
- **Cognition** — Processes such as memory, reasoning, and problem solving, often described at the psychological level.  
- **Double dissociation** — A neuropsychological method where complementary patterns of impairment suggest independence of functions.  
- **Emergence** — The appearance of novel properties when many simple components interact.  
- **Internal model** — A learned mapping that predicts how the world and body respond to actions.  
- **Map vs. territory** — The idea that representations useful for behavior are not the same as the mechanisms that generate behavior.  
- **Metaphor (in neuroscience)** — Borrowed imagery from technology used to explain brain function (hydraulic, clockwork, computer).  
- **Reductionism** — The view that higher-level theories must remain consistent with lower-level biological and physical principles.  
- **Von Neumann architecture** — A computer design separating memory, processing, and control, with data transferred along buses.  

---

## Suggested Readings

- Baddeley, A. D. (2012). *Working Memory: Theories, Models, and Controversies*. Annual Review of Psychology, 63, 1–29.  
- Shallice, T. (1988). *From Neuropsychology to Mental Structure*. Cambridge University Press.  
- Bechtel, W. (2008). *Mental Mechanisms: Philosophical Perspectives on Cognitive Neuroscience*. Routledge.  
- Dupuy, J.-P. (2009). *On the Origins of Cognitive Science: The Mechanization of the Mind*. MIT Press.  
- Gibson, J. J. (1979). *The Ecological Approach to Visual Perception*. Houghton Mifflin.  