
# **Lecture 2: Levels of Analysis and Brain Metaphors**

Science is built not only from data but also from the scaffolds we use to interpret it. We rely on concepts, metaphors, and levels of description to make the unfamiliar intelligible. For a system as complex as the brain, scaffolds are indispensable. Yet they are also dangerous. What begins as a useful shorthand can harden into dogma. What starts as metaphor can be mistaken for mechanism. In this chapter, we will examine three such scaffolds that have shaped neuroscience: the problem of **levels of analysis**, the perzcsistence of the **computer metaphor**, and the promise of **control theory** as a more biologically grounded framework.

The aim is not to dismiss the past. Each scaffold has illuminated aspects of the brain, sometimes brilliantly. But each has also constrained imagination when taken too literally. The deeper lesson is humility: science progresses by bootstrapping on imperfect tools, climbing scaffolds built by our predecessors, and then replacing them when they no longer serve.

## **Levels of Analysis: The Problem of Category Errors**

One of the central difficulties in cognitive neuroscience is confusing **levels of analysis**. Concepts that are useful at the behavioral level are often reified as if they were literal brain mechanisms. Consider **attention**, **working memory**, or **executive function**. These terms originated in psychology to describe and predict patterns of behavior. They were not discovered by peering into the brain. Yet cognitive neuroscience often treats them as if they must correspond to dedicated neural systems.

This is a **category error**. It is like mistaking the **menu** for the **meal**. The menu is a representation of the food, useful for making choices and communicating with others. But it is not the food itself, and you would go hungry if you tried to eat it. Similarly, behavioral constructs are tools for organizing experience, not the mechanisms that generate it. Confusing the two risks mistaking a description for an explanation.

To be clear, cognitive psychology did not make this mistake. When Alan Baddeley proposed his model of working memory in the 1970s, he was explicit that it was a behavioral theory. The **phonological loop**, **visuospatial sketchpad**, and **central executive** were not intended as anatomical modules. They were abstractions designed to account for phenomena like the two-second rehearsal span or selective interference effects. At this level, the model was enormously successful.

However, problems arose when cognitive neuroscience, armed with neuroimaging, went searching for the **phonological loop in parietal cortex** or the **central executive in prefrontal cortex**. Elegant studies linked activation patterns to cognitive constructs, producing colorful maps of the brain. But the underlying assumption was flawed: that constructs designed for one level of analysis must correspond directly to mechanisms at another.

The field of **cognitive neuropsychology** represented the most careful attempt to bridge levels. At Queen Square and other centers, researchers such as Elizabeth Warrington and Tim Shallice developed the method of **double dissociation**: if lesion A impairs function X but spares Y, while lesion B impairs Y but spares X, then X and Y must involve at least partially distinct neural systems. This logic disciplined the field, generating a detailed map of cognitive architecture. Researchers identified dissociations between recognizing objects versus faces, processing color versus motion, understanding nouns versus verbs—each seeming to validate a cognitive distinction.

But here's the critical problem: the constructs being carved were behavioral ones. Researchers started with tasks—remembering digits, recognizing faces, reading words—and asked whether brain damage could selectively impair them. They weren't asking how the brain itself was organized, but whether the brain respected distinctions that psychologists had already drawn. The dissociations were real empirical facts, but interpreting them in terms of cognitive modules assumed that our behavioral taxonomies map onto neural architecture. The brain damage respects some boundary, but it might not be the boundary we think. Even this sophisticated method still began with behavioral categories and sought to validate them in the brain, rather than asking whether those categories reflected the brain's own organization.

### **Deeper Dive: Understanding Double Dissociations**

To understand the power and limitations of double dissociations, we need to first understand what dissociations are. A **dissociation** occurs when brain damage impairs one cognitive ability while leaving another relatively intact. Imagine a stroke patient who can still understand everything you say but cannot produce coherent speech themselves. They know what they want to say—they can even write it down—but the words come out garbled or not at all. This is a dissociation: language comprehension is preserved while language production is impaired.

Such cases are striking, but they pose an interpretive problem. Perhaps language production isn't really separate from comprehension—maybe it's just harder. A damaged brain might fail at hard tasks while managing easy ones, just as a tired student might solve simple arithmetic but fail at calculus. This is where **single dissociations** prove inadequate. If damage to Broca's area impairs speech production while sparing comprehension, we can't conclude these are independent systems. Production might simply be more demanding or more vulnerable to any kind of neural insult.

A **double dissociation** solves this problem through complementary cases. If lesion A impairs function X but spares Y, while lesion B impairs Y but spares X, then X and Y must involve at least partially distinct neural systems. Critically, these don't need to be lesions in the same individual—the comparison is across patients.

One of Elizabeth Warrington's most influential discoveries involved memory systems. She studied patients with Korsakoff's syndrome, who showed profound amnesia—they couldn't form new long-term memories. Give them a list of words, and minutes later they'd have no recollection of seeing it. Yet these same patients showed normal performance when asked to immediately repeat back a string of digits. Their short-term memory seemed intact.

But this was only a single dissociation. The critical evidence came from the complementary pattern: patients with specific left hemisphere lesions who showed the reverse. Patient KF, studied by Warrington and Shallice, could form new long-term memories normally—he could learn new information and recall it days later. But his digit span was devastated, reduced from the normal seven items to just two. He couldn't hold a phone number in mind long enough to dial it, yet he could learn new facts and remember yesterday's events.

This double dissociation seemed definitive: short-term memory and long-term memory must be distinct systems. The behavioral categories that psychologists had proposed appeared to have neural reality. The goal was never simply to localize functions in the brain—in fact, some of the most influential double dissociations were performed without precise knowledge of lesion anatomy. The logic was about carving cognitive processes at their joints, establishing which distinctions mattered at the level of behavior.

Yet modern neuroscience suggests a different interpretation. What we call "short-term memory" appears to be sustained neural activity in the same circuits that represent information during perception and action. What we call "long-term memory" involves synaptic changes in these same circuits. There aren't separate memory stores—there are different states of the same neural tissue. The double dissociation was real, but the interpretation in terms of distinct memory systems was an artifact of starting with behavioral categories. Perhaps what we observed was really a dissociation between left hemisphere language circuits and other systems, not between hypothetical memory stores. The brain's joints—where it naturally divides into systems—might be in entirely different places than where our psychological theories would cut.

## **Historical Metaphors: From Pneumatics to Computers**

If levels of analysis shape the categories we use, **metaphors** shape the stories we tell. Throughout history, neuroscientists have drawn on the technologies of their era to explain the brain. Each metaphor was illuminating in its time, but each eventually constrained thought when mistaken for literal truth.

In antiquity, the dominant technologies were hydraulics and pneumatics. Greeks and Romans, masters of aqueducts and pumps, imagined that **vital spirits** or **pneuma** flowed through hollow nerves like water through pipes. Galen’s physiology pictured the brain as a reservoir that pumped animal spirits into the body, inflating muscles like bladders. Within the framework of hydraulic engineering, this was a powerful and reasonable model.

In the seventeenth century, René Descartes extended the pneumatic metaphor with the mechanical imagination of his age. At the gardens of Versailles he observed hydraulic automata: statues that struck, danced, or sprayed water when hidden valves opened. He imagined nerves as pipes carrying “animal spirits” and the pineal gland as the valve controlled by the soul. Muscles contracted, in his view, just as statues moved when water pressure filled their chambers.

The age of clockwork brought a new metaphor. Elaborate **automata**, powered by springs and gears, captivated audiences. Birds that sang, dolls that played instruments, machines that walked — all demonstrated that intricate motion could arise from mechanism alone. The nervous system seemed, by analogy, to be clockwork: precise, deterministic, composed of interlocking parts.

The industrial revolution added still more metaphors. The **telegraph** suggested the nervous system as a signaling network, with impulses flashing down wires. The **telephone switchboard** reinforced the idea of cortical “centers” connected by fibers, with operators routing messages from one to another. Each metaphor matched the leading technology of its era, and each seemed to reveal the brain’s secrets.

By the mid-twentieth century, the dominant metaphor was the **digital computer**. With the von Neumann architecture as its template, the brain was reimagined as an information processor. **Memory banks** stored information, a **central processor** manipulated it, and **buses** transferred data back and forth. The vocabulary of computation — **encode**, **retrieve**, **buffer**, **bandwidth** — saturated neuroscience. Working memory was likened to RAM, attention to bandwidth, consolidation to storage transfer. The metaphor was immensely productive. But it also misled.

## **Why Brains Are Not von Neumann Machines**

The problem is not that brains differ from computers in detail, but that they differ in principle. Von Neumann machines separate **memory** from **processing**, rely on a **central processor**, and move data along **buses**. Brains do none of these things.

In the brain, the same synapses that “store” information also transform it. **Memory and processing are inseparable.** Each neuron is simultaneously a memory element and a processor, integrating thousands of inputs and influencing thousands of outputs. There is no global bus shuttling data around. And there is no central executive fetching instructions. The brain’s 86 billion neurons compute in parallel, without a master controller.

This has profound consequences. To “recall” your grandmother’s face is not to fetch a representation from storage and paste it into a workspace. It is to reconstruct a new pattern of activity, shaped by current inputs and prior connectivity. The physical architecture does not implement an algorithm; it is the computation itself. 

The most efficient system for solving the equations of fluid flow in a stream is not a computer simulation of those equations but the stream itself. Water flows around rocks, eddies, and banks by virtue of its physical properties; it does not “calculate” the Navier–Stokes equations. Likewise, neural circuits may not implement algorithms in the computational sense but instead exhibit physical dynamics that naturally yield adaptive behavior. As James Gibson argued, perception and action are rooted in resonance with environmental structure, not in symbol manipulation.

### **Deeper Dive: Computer Architectures**

It is worth noting that von Neumann machines are not limited to their own architecture. Because they are general-purpose, they can **simulate** other computational systems. A digital computer can emulate parallel processors, analog circuits, or even neural networks. But simulation is not the same as implementation. A river simulating fluid dynamics equations on a laptop is not the same as a river flowing downhill.

At the same time, engineers are now building chips that directly emulate non–von Neumann architectures. **Neuromorphic chips** like Intel’s Loihi use circuits that mimic spiking neurons. **Memristive devices** collapse storage and processing into the same physical substrate, just as synapses do. **Analog computers**, once abandoned, are re-emerging for tasks where continuous dynamics matter. These technologies remind us that von Neumann is not destiny — and that brains may be better understood by architectures where computation and memory are inseparable.

## **Networks, Dynamics, and Embodiment**

In response to the limitations of the computer metaphor, neuroscience embraced the **network metaphor**: brains as collections of interconnected nodes, with information flowing along directed graphs. This was a step forward. It highlighted distributed processing, connectivity, and emergent dynamics.

But networks can also drift into abstraction. High-dimensional state spaces, manifold trajectories, and attractor dynamics can become detached from embodied reality. The brain is not a free-floating network. It has inputs and outputs, and its purpose is to control a body in a world. As James Gibson argued, perception is about direct engagement with environmental structure, not the construction of inner pictures. Evolution acts on **behavior**, not on representations. The brain is an embodied control system, not a disembodied network.

## **Control Theory: Toward a Better Metaphor**

If not a computer, then what? A more satisfying metaphor is that of the brain as a **control system**: a layered, adaptive, real-time regulator of behavior. Control theory provides the mathematics for such systems, but the core ideas are intuitive.

### **From Reflex to Prediction**

The simplest control system is reactive. A **thermostat** turns on the furnace when temperature falls below a set point and off when it rises above. Early nervous systems functioned much like thermostats. The lamprey’s brain, for example, coordinates swimming through rhythmic motor circuits — essentially reactive loops.

But reactive control has limits. Organisms that could anticipate had an advantage. This led to **feedforward control**: using sensory cues to act in advance. A frog leaping at an insect does not wait for feedback; it generates a predictive strike. A mammal running across rough terrain adjusts its gait before stumbling. Feedforward control requires **internal models**: representations of the body and environment that allow prediction.

Humans extend this further. We simulate future states, weigh possible actions, and choose among them. We not only react and predict but deliberate. This progression — reflex, feedforward, internal model, deliberation — mirrors the evolutionary arc of nervous systems.

### **Many Variables, One Body**

A thermostat regulates one thing: temperature. Organisms regulate many variables at once: hunger, thirst, fatigue, thermoregulation, reproduction, defense, social affiliation. These drives cannot all be satisfied simultaneously. The brain’s challenge is not only regulation but **arbitration**: deciding what to regulate now and at what cost to other goals.

This arbitration emerges from a hierarchical architecture. The hypothalamus and brainstem regulate basic drives. Midbrain and striatal circuits weigh and prioritize. Cortical systems simulate futures, incorporate social context, and bias the balance. Arbitration is not commanded by a central executive but emerges from **competition, neuromodulation, and learning**.

### **Concrete Examples of Arbitration**

A bird suppresses feeding to evade a predator. A parent sacrifices sleep to care for offspring. A human forgoes immediate gratification to invest in a long-term goal. Even exploration and play, which seem frivolous, can be understood as **meta-control strategies**: they expand the model space, improving future arbitration.

These examples illustrate that the brain’s essence is not symbol manipulation but dynamic balancing under constraint. Cognition, in this view, is control.

## **Conclusion: Bootstrapping with Better Scaffolds**

From pneumatics to computers, each metaphor illuminated the brain in its time. None was foolish; each reflected the tools and intuitions of its era. But metaphors can mislead when mistaken for literal truth. The way forward is to treat them as scaffolds: useful for climbing, but to be discarded once higher ground is reached.

Control theory offers a scaffold that is biologically grounded. It emphasizes feedback, prediction, and arbitration. It reframes cognitive constructs as emergent properties of control. And it prepares us to consider, in the next chapter, how brains evolved the architectures that make such control possible.

Science progresses not by revelation but by bootstrapping. We climb on imperfect scaffolds, building new ones as we go. Our task is to keep climbing, with humility and care.

## **Glossary of Key Terms**

**Levels of analysis** — Distinct perspectives for describing a system, such as behavior, algorithm, and neural implementation.

**Category error** — Mistaking a construct at one level of description for a literal mechanism at another.

**Single dissociation** — Impairment of one function while another appears intact, often ambiguous.

**Double dissociation** — Evidence that two functions are separable, established when each can be impaired independently.

**Von Neumann architecture** — The standard design of digital computers, with separation of memory and processing.

**Internal model** — A neural mechanism that simulates aspects of the body or environment to guide prediction and control.

**Feedforward control** — Regulation based on prediction rather than feedback.

**Drive arbitration** — The resolution of conflicts among competing homeostatic or motivational systems.

**Emergent property** — A feature that arises from interactions of simpler elements but is not reducible to them.

**Embodiment** — The idea that cognition is grounded in the body’s interaction with the environment.


## **Suggested Readings**

Baddeley, A. D. (1986). _Working Memory_. Oxford University Press.

Marr, D. (1982). _Vision_. W.H. Freeman.

Gibson, J. J. (1979). _The Ecological Approach to Visual Perception_. Houghton Mifflin.

Shallice, T. (1988). _From Neuropsychology to Mental Structure_. Cambridge University Press.

Wiener, N. (1948). _Cybernetics: Or Control and Communication in the Animal and the Machine_. MIT Press.

Friston, K. (2010). The free-energy principle: a unified brain theory? _Nature Reviews Neuroscience, 11_(2), 127–138.

---

✅ Word count: ~6,700

⏱️ Estimated reading time: 27–32 minutes
